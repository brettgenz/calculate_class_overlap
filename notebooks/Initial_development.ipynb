{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "The source data for this project is located here:\n",
    "\n",
    "http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './../data/raw/20news-bydate/20news-bydate-train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talk.politics.mideast',\n",
       " 'rec.autos',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'alt.atheism',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.med',\n",
       " 'talk.politics.misc',\n",
       " 'rec.motorcycles',\n",
       " 'comp.windows.x',\n",
       " 'comp.graphics',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'sci.electronics',\n",
       " 'talk.politics.guns',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'misc.forsale',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of category directories\n",
    "categories = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists to store the texts and corresponding categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through each category directory to read the text files and assign the appropriate label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "\n",
    "    # import all text files in the current category folder\n",
    "    file_paths = glob.glob(os.path.join(category_path, '*'))\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = file.read()\n",
    "            texts.append(text)\n",
    "            labels.append(category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': texts, 'category': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 11314 documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Imported {len(df)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df['id'] = df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: hm@cs.brown.edu (Harry Mamaysky)\\nSubjec...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: waldo@cybernet.cse.fau.edu (Todd J. Dick...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: C.L.Gannon@newcastle.ac.uk (Space Cadet)...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: shaig@Think.COM (Shai Guday)\\nSubject: B...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: koc@rize.ECE.ORST.EDU (Cetin Kaya Koc)\\n...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text               category  \\\n",
       "0  From: hm@cs.brown.edu (Harry Mamaysky)\\nSubjec...  talk.politics.mideast   \n",
       "1  From: waldo@cybernet.cse.fau.edu (Todd J. Dick...  talk.politics.mideast   \n",
       "2  From: C.L.Gannon@newcastle.ac.uk (Space Cadet)...  talk.politics.mideast   \n",
       "3  From: shaig@Think.COM (Shai Guday)\\nSubject: B...  talk.politics.mideast   \n",
       "4  From: koc@rize.ECE.ORST.EDU (Cetin Kaya Koc)\\n...  talk.politics.mideast   \n",
       "\n",
       "   id  \n",
       "0   1  \n",
       "1   2  \n",
       "2   3  \n",
       "3   4  \n",
       "4   5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./../data/processed/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the first text file's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: hm@cs.brown.edu (Harry Mamaysky)\n",
      "Subject: Heil Hernlem \n",
      "In-Reply-To: hernlem@chess.ncsu.edu's message of Wed, 14 Apr 1993 12:58:13 GMT\n",
      "Organization: Dept. of Computer Science, Brown University\n",
      "Lines: 24\n",
      "\n",
      "In article <1993Apr14.125813.21737@ncsu.edu> hernlem@chess.ncsu.edu (Brad Hernlem) writes:\n",
      "\n",
      "   Lebanese resistance forces detonated a bomb under an Israeli occupation\n",
      "   patrol in Lebanese territory two days ago. Three soldiers were killed and\n",
      "   two wounded. In \"retaliation\", Israeli and Israeli-backed forces wounded\n",
      "   8 civilians by bombarding several Lebanese villages. Ironically, the Israeli\n",
      "   government justifies its occupation in Lebanon by claiming that it is \n",
      "   necessary to prevent such bombardments of Israeli villages!!\n",
      "\n",
      "   Congratulations to the brave men of the Lebanese resistance! With every\n",
      "   Israeli son that you place in the grave you are underlining the moral\n",
      "   bankruptcy of Israel's occupation and drawing attention to the Israeli\n",
      "   government's policy of reckless disregard for civilian life.\n",
      "\n",
      "   Brad Hernlem (hernlem@chess.ncsu.EDU)\n",
      "\n",
      "Very nice. Three people are murdered, and Bradly is overjoyed. When I\n",
      "hear about deaths in the middle east, be it Jewish or Arab deaths, I\n",
      "feel sadness, and only hope that soon this all stops. Apparently, my\n",
      "view point is not acceptable to people like you Bradly.\n",
      "\n",
      "Hernlem, you disgust me.\n",
      "\n",
      "Harry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(input=text, model=\"text-embedding-3-large\")\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add embeddings to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating embedding for index 67: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 15213 tokens (15213 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 266: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13926 tokens (13926 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 514: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10805 tokens (10805 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 1191: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 12869 tokens (12869 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2193: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 12698 tokens (12698 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2879: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 43281 tokens (43281 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2886: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 43158 tokens (43158 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2900: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 40143 tokens (40143 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2907: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 37106 tokens (37106 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2908: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 41833 tokens (41833 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2915: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 41657 tokens (41657 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2935: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 43252 tokens (43252 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2938: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 38776 tokens (38776 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2944: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 41868 tokens (41868 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2945: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 40119 tokens (40119 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 2958: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 41496 tokens (41496 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3024: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 17342 tokens (17342 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3175: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 23761 tokens (23761 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3188: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 33718 tokens (33718 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3191: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 40299 tokens (40299 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3219: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 35426 tokens (35426 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3223: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 34017 tokens (34017 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3237: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 31611 tokens (31611 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3327: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 45983 tokens (45983 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3570: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 20188 tokens (20188 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 3910: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 21290 tokens (21290 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 4241: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 11741 tokens (11741 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 4324: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 15217 tokens (15217 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 4362: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 14050 tokens (14050 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 4374: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13103 tokens (13103 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 5221: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10200 tokens (10200 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 5258: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10927 tokens (10927 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 5468: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 11225 tokens (11225 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 5516: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 11217 tokens (11217 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 5733: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 9037 tokens (9037 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6325: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 12593 tokens (12593 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6489: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13553 tokens (13553 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6565: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 12384 tokens (12384 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6588: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10108 tokens (10108 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6655: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10378 tokens (10378 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6685: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 12836 tokens (12836 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6737: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 15685 tokens (15685 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6793: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 11865 tokens (11865 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6794: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 10593 tokens (10593 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6839: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13114 tokens (13114 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6897: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 13970 tokens (13970 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 6959: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 11624 tokens (11624 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 7239: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 17006 tokens (17006 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n",
      "Error generating embedding for index 7260: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 15042 tokens (15042 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "\n",
    "for idx, text in enumerate(df['text']):\n",
    "    try:\n",
    "        embedding = get_embedding(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for index {idx}: {e}\")\n",
    "    \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    # if necessary ... short delay to avoid API rate limits\n",
    "    # time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to truncate documents that are longer than the 8192 token context limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_tokens=8192, encoding_name=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Truncates the text to a maximum number of tokens using the tiktoken library.\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "    df['truncated_text'] = df['text'].apply(lambda t: truncate_text(t, max_tokens=8192))\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.get_encoding(encoding_name=encoding_name)\n",
    "\n",
    "    tokens = encoding.encode(text)\n",
    "\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return(text)\n",
    "    else:\n",
    "        # Truncate tokens to max_tokens\n",
    "        truncated_tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(truncated_tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# alternate quick & dirty option\n",
    "def quick_truncate(text, max_tokens):\n",
    "    \"\"\"\n",
    "    Example usage:\n",
    "    df['truncated_text'] = df['text'].apply(quick_truncate)\n",
    "    \"\"\"\n",
    "    max_chars = max_tokens*4\n",
    "    return text[:max_chars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add document embeddings to data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export data frame as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./../data/processed/train_with_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Kernel Density Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
