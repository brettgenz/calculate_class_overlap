{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import embedding_functions\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "data_path = './../data/raw/20news-bydate/20news-bydate-train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of category directories\n",
    "categories = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "\n",
    "# create lists to store texts and corresponding categories\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each category directory to read the text files and assign the appropriate label.\n",
    "for category in categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "\n",
    "    # import all text files in the current category folder\n",
    "    file_paths = glob.glob(os.path.join(category_path, '*'))\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = file.read()\n",
    "            texts.append(text)\n",
    "            labels.append(category)\n",
    "\n",
    "# create data frame\n",
    "df = pd.DataFrame({'text': texts, 'category': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for idx, text in enumerate(df['text']):\n",
    "    try:\n",
    "        embedding = get_embedding(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for index {idx}: {e}\")\n",
    "    \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "    # if necessary ... short delay to avoid API rate limits\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "# add embeddings to data frame\n",
    "df['embedding'] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Import Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('./../data/processed/train_with_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct Principal Component Analysis\n",
    "\n",
    "First, convert embeddings to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_array = np.array(df['embedding'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize and fit PCA to reduce to 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2d = PCA(n_components=2)\n",
    "\n",
    "components = pca_2d.fit_transform(embedding_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new components as X and Y columns in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x_2d'] = components[:, 0]\n",
    "df['y_2d'] = components[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, repeat PCA but for 3 dimensions, and add those to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "\n",
    "components = pca_3d.fit_transform(embedding_array)\n",
    "\n",
    "df['x_3d'] = components[:, 0]\n",
    "df['y_3d'] = components[:, 1]\n",
    "df['z_3d'] = components[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the final data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data frame as pickle file in 'processed' folder\n",
    "df.to_pickle('./../data/processed/train_with_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Overlap Matrix\n",
    "\n",
    "##### Generate Kernel Density Estimates\n",
    "\n",
    "Group the data by category and convert the 2D coordinates to a 2 x n array for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdes = {}\n",
    "\n",
    "categories = df['category'].unique()\n",
    "\n",
    "for cls in categories:\n",
    "    subset = df[df['category'] == cls]\n",
    "\n",
    "    # create a 2 x n_points array with x and y coordinates\n",
    "\n",
    "    points = np.vstack([subset['x_2d'], subset['y_2d']])\n",
    "\n",
    "    kdes[cls] = gaussian_kde(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a grid over the embedding space using the `x_2d` and `y_2d` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = df['x_2d'].min() - 1, df['x_2d'].max() + 1\n",
    "ymin, ymax = df['y_2d'].min() - 1, df['y_2d'].max() + 1\n",
    "\n",
    "grid_size = 100  # resolution of the grid\n",
    "xgrid = np.linspace(xmin, xmax, grid_size)\n",
    "ygrid = np.linspace(ymin, ymax, grid_size)\n",
    "X, Y = np.meshgrid(xgrid, ygrid)\n",
    "grid_coords = np.vstack([X.ravel(), Y.ravel()])  # shape: (2, grid_size^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate each class' KDE on the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = {}\n",
    "\n",
    "for cls, kde in kdes.items():\n",
    "    # evaluate the KDE on the grid and reshape back to grid form\n",
    "\n",
    "    density[cls] = kde(grid_coords).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute KDE Overlap Coefficient\n",
    "\n",
    "Determine the area element (dx * dy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = xgrid[1] - xgrid[0]\n",
    "dy = ygrid[1] - ygrid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over all pairs of classes using sorted category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_results = {}\n",
    "\n",
    "classes = sorted(density.keys())\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(i + 1, len(classes)):\n",
    "        cls1 = classes[i]\n",
    "        cls2 = classes[j]\n",
    "\n",
    "        # compute the pointwise minimum of the two density grids\n",
    "        min_density = np.minimum(density[cls1], density[cls2])\n",
    "\n",
    "        # approximate the integral over the grid as the sum of the minimum density\n",
    "        overlap_coef = np.sum(min_density) * dx * dy\n",
    "\n",
    "        overlap_results[(cls1, cls2)] = overlap_coef\n",
    "\n",
    "\n",
    "classes = sorted(df['category'].unique())\n",
    "n_classes = len(classes)\n",
    "\n",
    "overlap_matrix = np.full((n_classes, n_classes), np.nan)\n",
    "\n",
    "# fill the matrix with the computed overlaps\n",
    "for (cls1, cls2), coef in overlap_results.items():\n",
    "    # find the index for each class in the sorted list\n",
    "    i = classes.index(cls1)\n",
    "    j = classes.index(cls2)\n",
    "\n",
    "    overlap_matrix[i, j] = coef\n",
    "    # overlap_matrix[j, i] = coef # optional: leaving this out for now to create an upper diagonal matrix\n",
    "\n",
    "# set the diagonal to 1\n",
    "# np.fill_diagonal(overlap_matrix, 1) # skipping this for now as the diagonal grabs too much attention\n",
    "\n",
    "# create DF with labels for the heat map\n",
    "df_overlap = pd.DataFrame(overlap_matrix,\n",
    "                          index=[f\"{c}\" for c in classes],\n",
    "                          columns=[f\"{c}\" for c in classes]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Overlap Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overlap.to_csv('./../data/processed/df_overlap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
